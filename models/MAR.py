import math
from functools import partial
import numpy as np

import torch
import torch.nn as nn

import math

import torch

from logging import getLogger
try:
    from diffloss import DiffLoss
except:
    from models.diffloss import DiffLoss

logger = getLogger()
CHANNEL_DICT = {k.upper():v for v,k in enumerate(
                     [
            'Fp1', 'AF7', 'AF3', 'F1', 'F3', 'F5', 'F7', 'FT7', 'FC5', 'FC3', 'FC1', 'C1', 'C3', 'C5', 'T7','TP7', 'CP5', 'CP3', 'CP1', 'P1', 'P3', 'P5', 'P7', 'P9', 'PO7', 'PO3', 'O1', 'Iz', 'Oz', 'POz','Pz', 'CPz', 'Fpz', 'Fp2', 'AF8', 'AF4', 'AFz', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT8', 'FC6', 'FC4', 'FC2', 'FCz', 'Cz', 'C2', 'C4', 'C6', 'T8', 'TP8', 'CP6', 'CP4', 'CP2', 'P2', 'P4', 'P6', 'P8', 'P10', 'PO8', 'PO4', 'O2', 'Fp1h', 'Fp2h', 'AF1', 'AF2', 'AF5', 'AF6', 'F9', 'F10', 'FT9', 'FT10', 'TP9', 'TP10', 'P9h', 'P10h', 'F1h', 'F2h', 'F5h', 'F6h', 'F7h', 'F8h', 'FC1h', 'FC2h', 'FC5h', 'FC6h', 'FT1', 'FT2', 'C1h', 'C2h', 'C5h', 'C6h', 'T1', 'T2', 'TP1', 'TP2', 'CP1h', 'CP2h', 'CP5h','CP6h', 'TP3', 'TP4', 'P1h', 'P2h', 'P3h', 'P4h', 'PO1', 'PO2', 'PO5', 'PO6', 'O9', 'O10', 'FT7h','FT8h', 'TP7h', 'TP8h', 'PO9', 'PO10', 'Iz2', 'Oz2', 'Pz2', 'CPz2', 'TPP9h', 'TPP10h', 'AFF1', 'AFF2', 'FFC5h', 'FFC3h', 'FFC4h', 'FFC6h', 'FCC5h', 'FCC3h', 'FCC4h', 'FCC6h', 'CCP5h', 'CCP3h', 'CCP4h','CCP6h', 'CPP5h', 'CPP3h', 'CPP4h', 'CPP6h', 'PPO1', 'PPO2', 'I1', 'I2','AFp3h', 'AFp4h', 'AFF5h', 'AFF6h', 'FFT7h', 'FFC1h', 'FFC2h', 'FFT8h','FTT9h', 'FTT7h', 'FCC1h', 'FCC2h', 'FTT8h', 'FTT10h', 'TTP7h', 'CCP1h', 'CCP2h', 'TTP8h', 'TPP7h', 'CPP1h', 'CPP2h','TPP8h','PPO9h', 'PPO5h','PPO6h', 'PPO10h', 'POO9h', 'POO3h', 'POO4h', 'POO10h', 'OI1h', 'OI2h', 'T3', 'T4', 'T9', 'T10', 'AFp1', 'AFp2', 'AFF1h', 'AFF2h', 'PPO1h', 'POO1', 'POO2', 'PPO2h', 'None'
         ])}

################################# Utils ######################################

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

################################ For reindex of Tokens #######################
def get_idxs(x_tid, summary_token_tid):
    """ 
    :params x_tid [B, mN]
    :params summary_token_tid [B, N*num_embed]
    """
    B = x_tid.shape[0]
    x_tid = x_tid.float()
    summary_token_tid = summary_token_tid.float()
    idx1 = x_tid + (torch.arange(0, x_tid.shape[1]).to(x_tid)  / (2 * x_tid.shape[1] + 1e2)).unsqueeze_(0).repeat((B,1))
    idx2 = summary_token_tid + (torch.arange(0, summary_token_tid.shape[1]).to(summary_token_tid)  / (2 * summary_token_tid.shape[1] + 1e2)).unsqueeze_(0).repeat((B,1))+0.5
    idx2 = idx2.squeeze(-1)
    raw_val  = torch.cat([idx1, idx2], dim=-1)
    idx_val, idx  = raw_val.sort(dim=-1)
    ridx_val, ridx = idx.sort(dim=-1)
    return raw_val, idx_val, idx, ridx_val, ridx

################################ For ATTENTION MASKS #######################
def get_attn_token_mask(idx_val, embed_num):
    """
    :param idx_val: tensor([[0.0000, 0.5000, 1.5091, 2.0094, 2.5182, 3.5273, 4.0189, 4.5364],
        [0.5000, 1.0000, 1.5091, 2.0094, 2.5182, 3.5273, 4.0189, 4.5364]])
    :return attn_mask
    """
    idx_block = torch.floor(idx_val).int()
    max_idx = torch.max(idx_block).int().item()
    attn_mask = []
    token_mask = []
    for x in idx_block:
        rs = torch.tensor(list((i==x).sum() for i in range(max_idx+1)), dtype=torch.int)
        # attn_mask.append(torch.block_diag(*(torch.ones((i,i)) if i!=embed_num else torch.zeros((i,i)) for i in rs)).bool())
        attn_mask.append(torch.block_diag(*(torch.ones((i,i)) for i in rs)).bool())
        token_mask.append(rs!=embed_num)
    attn_mask = torch.stack(attn_mask, dim=0).unsqueeze_(1).to(idx_val.device)
    summary_token_mask = torch.stack(token_mask, dim=0).to(idx_val.device)
    summary_token_mask = summary_token_mask.repeat_interleave(embed_num, dim=-1)
    return attn_mask, summary_token_mask

def apply_mask_x(mask_x, x):
    """
    :param x: tensor of shape [B (batch-size), N (num-patches), C, D (feature-dim)]
    :param mask_x: tensor [B, mN, mC] containing indices of patches in [B, N, C] to keep 
    """    
    B, N, C, D = x.shape
    assert len(mask_x.shape)==2 # each sample in batch has its own mask
    mB, mNC = mask_x.shape
    assert mB==B
    mask_keep = mask_x.flatten(1).unsqueeze(-1).expand(-1, -1, D).to(x.device)
    
    masked_x = torch.gather(x.reshape((B, N*C, D)), dim=-2, index=mask_keep)
    masked_x = masked_x.contiguous().view((B,mNC,D))
    return masked_x

def apply_mask_y(mask_y, x):
    """
    :param x: tensor of shape [B (batch-size), N (num-patches), C, D (feature-dim)]
    :param mask_y: (B, mNC_y)  containing indices of patches in [B, N, C] to keep 
    """    
    B, N, C, D = x.shape
    assert len(mask_y.shape)==2
    mB, mNC_y = mask_y.shape
    assert mB==B
    mask_keep = mask_y.reshape((B,mNC_y,1)).repeat((1, 1, D))
    masked_x = torch.gather(x.reshape((B, N*C, D)), dim=-2, index=mask_keep)
    return masked_x

def make_masks(chs, num_patchs = (193,15), mNC_x=12, mNC_y=10):
    """
    parameters:
    - chs: channel map (B, 193)
    - num_patchs: (193, N)
    - mNC_x: number of patches to mask
    - mNC_y: number of patches as target
    
    returns:
    - mask_x: (B, mNC_x)  the indices of masked patch & channel
    - mask_y: (B, mNC_y)  the indices of masked patch & channel
    
    example:
        
    ```python
        B, C, N = 2, 3, 5
        num_patchs = (3, 5)
        chs = torch.tensor([[0,1,192],
                            [0,192,2],])
        # [[[0, 3, 6, 9, 12],
        #   [1, 4, 7, 10, 13],
        #   [2, 5, 8, 11, 14],],

        make_masks(chs, num_patchs, mNC_x=3, mNC_y=2)

        (tensor([[ 4,  6, 13],
         [ 2,  5,  8]]),
        tensor([[ 0,  9],
                [ 9, 12]]))
    ```
    """
    B, C = chs.shape
    
    C, N = num_patchs
    
    used_chan_mask = (chs != 192)  # B, C
    
    # -- mask channels --
    
    indices = ( torch.rand((B, N, C), device=chs.device) + 0.1 ) * ( used_chan_mask.float().unsqueeze(1) )
    
    indices = torch.sort(indices.flatten(1), dim=-1, descending=True).indices # B, NC
    
    mask_x = indices[:, :mNC_x]
    
    mask_x = torch.sort(mask_x, dim=-1, descending=False).values # B, N, mC_x
    
    # -- mask y --
    
    indices = torch.rand((B, N, C), device=chs.device) + 0.1
    
    indices[torch.logical_not(used_chan_mask).unsqueeze(1).expand(-1, N, -1)] = 0.0
    
    indices = indices.flatten(1)
    
    indices[np.arange(indices.shape[0])[:, None], mask_x.flatten(1)] = 0.0
    
    indices = torch.sort(indices, dim=-1, descending=True).indices # B, NC
    
    indices = indices[:, :mNC_y]
    
    mask_y = torch.sort(indices, dim=-1, descending=False).values # B, mNC_y
    
    return mask_x, mask_y

def make_target(x, mask_y, num_patches, use_norm=False):
    with torch.no_grad():
        C, N = num_patches
        assert x.shape[-1]%N==0 and x.shape[-2]%C == 0
        block_size_c, block_size_n = x.shape[-2]//C, x.shape[-1]//N
        x = x.view(x.shape[0], C, block_size_c, N, block_size_n)
        # 将维度重新排列以使分块沿着通道轴和空间轴
        x = x.permute(0, 3, 1, 2, 4).contiguous() # B, N, C, bc, bn
        x = x.view(x.shape[0], C, N, block_size_c * block_size_n)
        y = apply_mask_y(mask_y.to(x.device), x)
        if use_norm:
            y = torch.nn.functional.layer_norm(y, (y.size(-1),))
        return y

def make_sample(x, sample, mask_y, num_patches = (1, 49)):
    C, N = num_patches
    assert x.shape[-1]%N==0 and x.shape[-2]%C == 0
    block_size_c, block_size_n = x.shape[-2]//C, x.shape[-1]//N
    x = x.view(x.shape[0], C, block_size_c, N, block_size_n)
    # 将维度重新排列以使分块沿着通道轴和空间轴
    x = x.permute(0, 3, 1, 2, 4).contiguous() # B, N, C, bc, bn
    x = x.view(x.shape[0], C * N, block_size_c * block_size_n)
    x = x.contiguous().clone().detach()
    # print(x.shape, mask_y.shape, sample.shape)
    x[ torch.arange(x.shape[0], device=x.device) ,mask_y, :] = sample
    x = x.view(x.shape[0], C, N, block_size_c, block_size_n)
    x = x.permute(0, 1, 3, 2, 4).contiguous() # B, N, C, bc, bn
    x = x.view(x.shape[0], C * block_size_c, N * block_size_n).contiguous()
    return x

################################# RoPE Model Begin ######################################
# rotary embedding helper functions

def rotate_half(x):
    
    # x = rearrange(x, '... (d r) -> ... d r', r = 2)
    x = x.reshape((*x.shape[:-1],x.shape[-1]//2, 2))
    x1, x2 = x.unbind(dim = -1)
    x = torch.stack((-x2, x1), dim = -1)
    # return rearrange(x, '... d r -> ... (d r)')
    return x.flatten(-2)

def apply_rotary_emb(freqs, t, start_index = 0, scale = 1.):
    freqs = freqs.to(t)
    rot_dim = freqs.shape[-1]
    end_index = start_index + rot_dim
    assert rot_dim <= t.shape[-1], f'feature dimension {t.shape[-1]} is not of sufficient size to rotate in all the positions {rot_dim}'
    t_left, t, t_right = t[..., :start_index], t[..., start_index:end_index], t[..., end_index:]
    # print("t, freqs", t.shape, freqs.shape)
    # -- fixed head num repeats --
    freqs = freqs.unsqueeze(1).repeat((1, t.shape[1], 1, 1))
    t = (t * freqs.cos() * scale) + (rotate_half(t) * freqs.sin() * scale)
    return torch.cat((t_left, t, t_right), dim = -1)


class RotaryEmbedding(nn.Module):
    def __init__(
        self,
        dim,
        theta = 10000,
        learned_freq = False,
        interpolate_factor = 1.
    ):
        super().__init__()
        
        self.cache = dict()
        self.cache_scale = dict()
        self.freqs = nn.Parameter(
            1. / (theta ** (torch.arange(0, dim, 2)[:(dim // 2)].float() / dim)), 
            requires_grad = learned_freq)
        
        # interpolation factors

        assert interpolate_factor >= 1.
        self.interpolate_factor = interpolate_factor

        self.register_buffer('scale', None)
        
    def prepare_freqs(self, num_patches = (1, 8), device='cuda', dtype=torch.float, offset = 0):
        """
        example usage:
        ```python
            time_embed = RotaryEmbedding(dim=4, interpolate_factor=1.0)
            time_embed.prepare_freqs((2,3), device='cpu').contiguous().view((3, 2, 4))
            # tensor([[[0.0000, 0.0000, 0.0000, 0.0000],
            #          [0.0000, 0.0000, 0.0000, 0.0000]],

            #         [[1.0000, 1.0000, 0.0100, 0.0100],
            #          [1.0000, 1.0000, 0.0100, 0.0100]],

            #         [[2.0000, 2.0000, 0.0200, 0.0200],
            #          [2.0000, 2.0000, 0.0200, 0.0200]]])
        ```
        """
        # num_patches (C, N)
        C, N = num_patches
        cache_key = f'freqs:{num_patches}'
        
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        seq_pos = torch.arange(N, device = device, dtype = dtype)
        seq_pos = seq_pos.repeat_interleave(repeats=C, dim=0) # correspond to x (B, N, C, D)
        seq_pos = (seq_pos + offset) / self.interpolate_factor
        
        freqs = self.freqs
        freqs = torch.outer(seq_pos.type(freqs.dtype), freqs) # (n_seq_pos, n_freqs)
        freqs = freqs.repeat_interleave(repeats=2, dim=-1)    # (n_seq_pos, n_freqs*2)

        self.cache[cache_key] = freqs

        return freqs
    

################################# Model Begin ######################################

class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).
    """
    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob
        
    def drop_path(self, x, drop_prob: float = 0., training: bool = False):
        if drop_prob == 0. or not training:
            return x
        keep_prob = 1 - drop_prob
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets
        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
        random_tensor.floor_()  # binarize
        output = x.div(keep_prob) * random_tensor
        return output
    
    def forward(self, x):
        return self.drop_path(x, self.drop_prob, self.training)

class MLP(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features 
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x

class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0., is_causal=False, use_rope=False, return_attention=False):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = dim // num_heads

        self.use_rope = use_rope
        
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
            
        self.attn_drop = attn_drop
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        self.is_causal = is_causal
        self.return_attention= return_attention

    def forward(self, x, freqs=None, attn_mask=None, token_mask=None):
        B, T, C = x.shape
        qkv = self.qkv(x).reshape(B, T, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) # 3,B,nh,t,d
        q, k, v = qkv[0], qkv[1], qkv[2] # B,nh,t,d
        
        if self.use_rope:# RoPE
            q = apply_rotary_emb(freqs, q)
            k = apply_rotary_emb(freqs, k)
        # efficient attention using Flash Attention CUDA kernels
        # print(q.shape,k.shape,v.shape, attn_mask.shape)
        if (attn_mask is not None) and (token_mask is not None):
            mask_1 = token_mask.unsqueeze(1).unsqueeze(2)
            # mask_2 = token_mask.unsqueeze(1).unsqueeze(-1)
            attn_mask = attn_mask & attn_mask.masked_fill(~mask_1, False)# & attn_mask.masked_fill(~mask_1, False)
            
        elif (token_mask is not None):
            mask_1 = token_mask.unsqueeze(1).unsqueeze(2)
            # mask_2 = token_mask.unsqueeze(1).unsqueeze(-1)
            attn_mask = torch.ones((B, q.shape[1], T, T)).bool().to(x.device)
            attn_mask = attn_mask.masked_fill(~mask_1, False)# & attn_mask.masked_fill(~mask_1, False)
            
        if (attn_mask is not None):
            attn_mask = attn_mask.to(x.device)
        y = torch.nn.functional.scaled_dot_product_attention(
            q, k, v, attn_mask=attn_mask, dropout_p=self.attn_drop if self.training else 0, is_causal=self.is_causal)
        x = y.transpose(1, 2).contiguous().view(B, T, C) #(B, nh, T, hs) -> (B, T, hs*nh)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

class Block(nn.Module):
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, is_causal=False, use_rope=False, return_attention=False):
        super().__init__()
        
        self.return_attention= return_attention
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop, is_causal=is_causal, use_rope=use_rope, return_attention = return_attention)
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

    def forward(self, x, freqs=None, attn_mask=None, token_mask=None):
        y = self.attn(self.norm1(x), freqs, attn_mask=attn_mask, token_mask=token_mask)
        if self.return_attention: return y
        x = x + self.drop_path(y)
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x

class PatchEmbed(nn.Module):
    """ Image to Patch Embedding
    """
    def __init__(self, img_size=(64, 1000), patch_size=16, patch_stride=None, embed_dim=768):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.patch_stride = patch_stride
        if patch_stride is None:
            self.num_patches = ((img_size[0]), (img_size[1] // patch_size))
        else:
            self.num_patches = ((img_size[0]), ((img_size[1] - patch_size) // patch_stride + 1))

        self.proj = nn.Conv2d(1, embed_dim, kernel_size=(1,patch_size), 
                              stride=(1, patch_size if patch_stride is None else patch_stride))
        
    def forward(self, x):
        # x: B,C,T
        x = x.unsqueeze(1)# B, 1, C, T
        x = self.proj(x).transpose(1,3) # B, T, C, D
        return x
    
class Reconstructor(nn.Module):
    def __init__(
        self,
        num_patches,
        img_size      = None,
        patch_size    = 64,
        chan_embed    = None,
        use_inp_embed = True,
        embed_dim     = 768,
        embed_num     = None,
        reconstructor_embed_dim=384,
        depth=6,
        num_heads=12,
        mlp_ratio=4.0,
        qkv_bias=True,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.0,
        norm_layer=nn.LayerNorm,
        init_std=0.02,
        interpolate_factor = 2.,
        return_attention_layer=-1,
        **kwargs
    ):
        super().__init__()
        self.use_inp_embed = use_inp_embed
        self.num_patches = num_patches
        
        if use_inp_embed:
            self.reconstructor_embed = nn.Linear(embed_dim, reconstructor_embed_dim, bias=True)
        
        
        self.mask_token          = nn.Parameter(torch.zeros(1, 1, reconstructor_embed_dim))
        
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
        # --
        self.time_embed_dim = (reconstructor_embed_dim//num_heads)//2
        self.time_embed = RotaryEmbedding(dim=self.time_embed_dim, interpolate_factor=interpolate_factor)
        if chan_embed is None:
            self.chan_embed = nn.Embedding(len(CHANNEL_DICT), reconstructor_embed_dim)
        else:
            self.chan_embed = chan_embed
        # --
        self.reconstructor_blocks = nn.ModuleList([
            Block(
                dim=reconstructor_embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, is_causal=False, use_rope=True, 
                return_attention=(i+1)==return_attention_layer)
            for i in range(depth)])
        self.reconstructor_norm = norm_layer(reconstructor_embed_dim)
        # ------
        self.init_std = init_std
        trunc_normal_(self.mask_token, std=self.init_std)
        self.apply(self._init_weights)
        self.fix_init_weight()
        

    def fix_init_weight(self):
        def rescale(param, layer_id):
            param.div_(math.sqrt(2.0 * layer_id))

        for layer_id, layer in enumerate(self.reconstructor_blocks):
            rescale(layer.attn.proj.weight.data, layer_id + 1)
            rescale(layer.mlp.fc2.weight.data, layer_id + 1)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=self.init_std)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            trunc_normal_(m.weight, std=self.init_std)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Embedding):
            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)
    
    def forward(self, x, chan_ids=None, summary_token_mask=None, mask_y=None, verbose=False):
        # conditions: (Nq, D) as qurey for downstream 
        # mask_x/mask_y: (mN, mC) one number index like (n*C+c) in matrix (N,C)
        
        # -- map from encoder-dim to pedictor-dim
        if self.use_inp_embed:
            x = self.reconstructor_embed(x)

        C, N        = self.num_patches
        B, N, eN, D= x.shape
        
        if chan_ids is None:
            chan_ids = torch.arange(0,C).unsqueeze(0)
        chan_ids = chan_ids.to(x).long()
        
        ############## Mask x ###############
        # -- get freqs for RoPE
        
        freqs_x      = self.time_embed.prepare_freqs((eN, N), x.device, x.dtype) # NC, time_dim
        freqs_x      = freqs_x.unsqueeze(0).repeat((B, 1, 1))
        
        ############# Mask y ################
        assert mask_y is not None, "mask_y should not be None"
        mask_y       = mask_y.to(x.device)
        
        # create query mask_token ys
        # -- add channels positional embedding to x
        chan_embed = self.chan_embed(chan_ids).unsqueeze(0) # (1,C) -> (1,1,C,D)
        N_y          = mask_y.shape[1]
        
        chan_embed   = chan_embed.repeat((B,N,1,1))
        chan_embed   = apply_mask_y(mask_y, chan_embed)
        
        freqs        = self.time_embed.prepare_freqs((C, N), x.device, x.dtype) # NC, time_dim
        freqs_y      = freqs.contiguous().view((1, N, C, self.time_embed_dim)).repeat((B, 1, 1, 1))
        freqs_y      = apply_mask_y(mask_y, freqs_y)        # 1, mN, mC, D
        freqs_y      = freqs_y.contiguous().view((B, N_y, self.time_embed_dim))
        
        y = self.mask_token.repeat((B, N_y, 1)) + chan_embed
            
        # -- concat query mask_token ys
        x           = x.flatten(1,2) # B N E D -> B NE D
        x           = torch.cat([x,y], dim=1)
        freqs_x     = torch.cat([freqs_x, freqs_y], dim=1).to(x)
        
        token_mask  = torch.cat([summary_token_mask.to(x.device), torch.ones(freqs_y.shape[:2]).to(x.device).bool()], dim=1)
        
        if verbose: print("token_mask", token_mask.int())
        
        # -- fwd prop
        for blk in self.reconstructor_blocks:
            x = blk(x, freqs_x, token_mask = token_mask) # B, NC, D
        
        x = x[:,-N_y:,:]      # B, N_y, D
        
        x = self.reconstructor_norm(x) 
        
        return x

class Encoder(nn.Module):
    def __init__(
        self,
        img_size=(64,1000),
        num_patches = None,
        patch_size  =64,
        patch_stride=None,
        chan_embed  = None,
        embed_dim=768,
        embed_num=1,
        reconstructor_embed_dim=384,
        depth=12,
        num_heads=12,
        mlp_ratio=4.0,
        qkv_bias=True,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.0,
        norm_layer=nn.LayerNorm,
        norm_out_layer=nn.LayerNorm,
        patch_module=PatchEmbed,# PatchNormEmbed
        init_std=0.02,
        return_attention_layer=-1,
        **kwargs
    ):
        super().__init__()
        self.num_features = self.embed_dim = embed_dim
        self.embed_num = embed_num
        
        self.num_heads = num_heads
        
        # --
        self.patch_embed = patch_module(
            img_size=img_size,
            patch_size=patch_size,
            patch_stride=patch_stride,
            embed_dim=embed_dim)
        self.num_patches = self.patch_embed.num_patches
        # --
        if chan_embed is None:
            self.chan_embed = nn.Embedding(len(CHANNEL_DICT), embed_dim)
        else:
            self.chan_embed = chan_embed
            
        # --
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule
        self.blocks = nn.ModuleList([
            Block(
                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, 
                is_causal=False, use_rope= False, return_attention=(i+1)==return_attention_layer)
            for i in range(depth)])
        self.norm = norm_out_layer(embed_dim)
        # ------
        self.init_std = init_std
        self.summary_token = nn.Parameter(torch.zeros(1, embed_num, embed_dim))
            
        trunc_normal_(self.summary_token, std=self.init_std)
        self.apply(self._init_weights)
        self.fix_init_weight()
        
    def prepare_chan_ids(self, channels):
        chan_ids = []
        for ch in channels:
            ch = ch.upper().strip('.')
            assert ch in CHANNEL_DICT
            chan_ids.append(CHANNEL_DICT[ch])
        return torch.tensor(chan_ids).unsqueeze_(0).long()
    
    def fix_init_weight(self):
        def rescale(param, layer_id):
            param.div_(math.sqrt(2.0 * layer_id))

        for layer_id, layer in enumerate(self.blocks):
            rescale(layer.attn.proj.weight.data, layer_id + 1)
            rescale(layer.mlp.fc2.weight.data, layer_id + 1)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=self.init_std)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            trunc_normal_(m.weight, std=self.init_std)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Embedding):
            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)

    def forward(self, x, chan_ids=None, mask_x=None, return_summary_token=True, verbose=False):
        # x.shape B, C, T
        # mask_x.shape mN, mC
        
        # -- patchify x
        x = self.patch_embed(x)
        B, N, C, D = x.shape
        
        x_tid = torch.arange(0, N, 1).to(x.device).reshape((1, N, 1, 1)).repeat((B, 1, C, 1))
        
        assert N==self.num_patches[1] and C==self.num_patches[0], f"{N}=={self.num_patches[1]} and {C}=={self.num_patches[0]}"
        
        if chan_ids is None:
            chan_ids = torch.arange(0,C).unsqueeze(0) 
        chan_ids = chan_ids.to(x).long()
        
        # -- add channels positional embedding to x
        x = x + self.chan_embed(chan_ids.long()).unsqueeze(0) # (1,C) -> (1,1,C,D)

        if mask_x is not None:
            mask_x = mask_x.to(x.device)
            x = apply_mask_x(mask_x, x)# B, mNC, D
            x_tid = apply_mask_x(mask_x, x_tid).squeeze_(-1)
        else:
            x = x.flatten(1,2)
            x_tid = x_tid.flatten(1)
            
        if verbose: print("x_tid", x_tid)
            
        # -- concat summary token
        summary_token = self.summary_token.unsqueeze(0).repeat((x.shape[0], N, 1, 1)).flatten(1,2)
        summary_token_tid = torch.arange(0, N).to(x.device).reshape((1, N, 1)).repeat((B, 1, self.embed_num)).flatten(1,2)
        
        raw_val, idx_val, idx, ridx_val, ridx = get_idxs(x_tid, summary_token_tid)
        
        if verbose: print("idx_val", idx_val)
        if verbose: print("idx", idx)
        if verbose: print("x_s_shape", x.shape, summary_token.shape)
        
        x = torch.cat([x, summary_token], dim=1)
        
        x = x[torch.arange(0,x.shape[0]).unsqueeze_(-1), idx]
        
        if verbose: 
            print("x_reindex", torch.arange(0,x.shape[0]*x.shape[1]).reshape(x.shape[:2])[torch.arange(0,x.shape[0]).unsqueeze_(-1), idx])
        
        attn_mask, summary_token_mask = get_attn_token_mask(idx_val, self.embed_num)
        
        if verbose: 
            print("attn_mask", attn_mask.int())
            print("summary_token_mask", summary_token_mask.int())
        
        # -- fwd prop
        
        for i, blk in enumerate(self.blocks):
            x = blk(x, attn_mask=attn_mask) # B, NC+?, D
        if verbose: 
            print("x_mean", x)

        x = x[torch.arange(0,x.shape[0]).unsqueeze_(-1), ridx]
        
        x = x[:, -summary_token.shape[1]:, :]
        
        if self.norm is not None:
            x = self.norm(x) 
            
        x = x.reshape((B, N, self.embed_num, -1))
        
        if return_summary_token:
            return x, summary_token_mask
        else:
            return x
    
class MAR(nn.Module):
    def __init__(
        self,
        img_size            =(10,250),
        patch_size          =50,
        embed_dim           =256,
        embed_num           =1,
        depth               =8,
        num_heads           =8,
        mlp_ratio           =4.0,
        qkv_bias            =True,
        drop_rate           =0.0,
        attn_drop_rate      =0.0,
        drop_path_rate      =0.0,
        norm_layer          =nn.LayerNorm,
        patch_module        =PatchEmbed,# PatchNormEmbed
        init_std            =0.02,
        
        # -- masking params
        mNC_x = 3,
        mNC_y = 10,
        
        # -- diffusion loss params
        diffusion_batch_mul = 4,
        diffloss_w          = 256,
        diffloss_d          = 4,
        num_sampling_steps  = 1000,
        grad_checkpointing  = False,
        
        # -- TEST
        test_zero_input     = False,
        **kwargs
    ):
        super().__init__()
        self.num_features = self.embed_dim = embed_dim
        self.test_zero_input = test_zero_input
        
        self.chan_embed = nn.Embedding(len(CHANNEL_DICT), embed_dim)
        self.encoder = Encoder(
            img_size            =img_size,
            patch_size          =patch_size,
            chan_embed          =self.chan_embed,
            embed_dim           =embed_dim,
            embed_num           =embed_num,
            depth               =depth,
            num_heads           =num_heads,
            mlp_ratio           =mlp_ratio,
            qkv_bias            =qkv_bias,
            drop_rate           =drop_rate,
            attn_drop_rate      =attn_drop_rate,
            drop_path_rate      =drop_path_rate,
            norm_layer          =norm_layer,
            norm_out_layer      =norm_layer,
            patch_module        =patch_module,
            init_std            =init_std,
            return_attention_layer=-1,
            )
        self.reconstructor = Reconstructor(
            num_patches         = self.encoder.num_patches,
            patch_size          = patch_size,
            chan_embed          = self.chan_embed,
            use_inp_embed       = True,
            embed_dim           = embed_dim,
            embed_num           = embed_num,
            reconstructor_embed_dim=embed_dim,
            depth               =depth,
            num_heads           =num_heads,
            mlp_ratio           =mlp_ratio,
            qkv_bias            =qkv_bias,
            drop_rate           =drop_rate,
            attn_drop_rate      =attn_drop_rate,
            drop_path_rate      =drop_path_rate,
            norm_layer          =norm_layer,
            init_std            =init_std,
            interpolate_factor  = 2.,
            return_attention_layer=-1,)
        
        
        self.diffloss = DiffLoss(
                target_channels = patch_size,
                z_channels      = embed_dim,
                width           = diffloss_w,
                depth           = diffloss_d,
                num_sampling_steps=num_sampling_steps,
                grad_checkpointing=grad_checkpointing
            )
        
        self.diffusion_batch_mul = diffusion_batch_mul
        
        self.num_patches = self.encoder.num_patches
        self.mNC_x = mNC_x; self.mNC_y = mNC_y
        
        
    def forward(self, x, chs=None, chan_ids=None):
        if chs is None:
            chs = torch.zeros(x.shape[:2]).long().to(x.device)
        
        mask_x, mask_y = make_masks(chs, num_patchs = self.num_patches, mNC_x=self.mNC_x, mNC_y=self.mNC_y)
        
        target      = make_target(x, mask_y, self.num_patches, use_norm=False)
        
        if self.test_zero_input:
            x = torch.zeros_like(x)
            
        z, z_mask   = self.encoder(x, chan_ids, mask_x=mask_x)
        
        z           = self.reconstructor(z, chan_ids, summary_token_mask=z_mask, mask_y=mask_y )
    
        bsz, seq_len, _ = target.shape
        target          = target.reshape(bsz * seq_len, -1).repeat(self.diffusion_batch_mul, 1)
        z               = z.reshape(bsz * seq_len, -1).repeat(self.diffusion_batch_mul, 1)
        
        with torch.cuda.amp.autocast():
            loss = self.diffloss(z=z, target=target)
        
        return loss
    
    def sample(self, x, chs=None, chan_ids=None, mask_x=None, mask_y=None):
        if chs is None:
            chs = torch.zeros(x.shape[:2]).long().to(x.device)
        
        if (mask_x is None) and (mask_y is None):
            mask_x, mask_y = make_masks(chs, num_patchs = self.num_patches, mNC_x=self.mNC_x, mNC_y=self.mNC_y)
        
        target      = make_target(x, mask_y, self.num_patches, use_norm=False)
        
        if self.test_zero_input:
            x = torch.zeros_like(x)
            
        z, z_mask   = self.encoder(x, chan_ids, mask_x=mask_x)
        
        z           = self.reconstructor(z, chan_ids, summary_token_mask=z_mask, mask_y=mask_y )
    
        bsz, seq_len, _ = target.shape
        target          = target.reshape(bsz * seq_len, -1).repeat(self.diffusion_batch_mul, 1)
        z               = z.reshape(bsz * seq_len, -1).repeat(self.diffusion_batch_mul, 1)
        # print(z.shape)
        with torch.cuda.amp.autocast():
            sample = self.diffloss.sample(z=z)
        # print(sample.shape)
        sample = make_sample(x, sample, mask_y, self.num_patches)
        
        return sample, mask_x, mask_y
    
if __name__=="__main__":
    x = torch.zeros((1,10,250)).float()
    
    model = MAR(
        img_size            =(10,250),
        patch_size          =50,
        embed_dim           =256,
        embed_num           =1,
        depth               =8,
        num_heads           =8,
        mlp_ratio           =4.0,
        qkv_bias            =True,
        drop_rate           =0.0,
        attn_drop_rate      =0.0,
        drop_path_rate      =0.0,
        norm_layer          =nn.LayerNorm,
        patch_module        =PatchEmbed,# PatchNormEmbed
        init_std            =0.02,
        
        # -- masking params
        mNC_x = 3,
        mNC_y = 3,
        
        # -- diffusion loss params
        diffloss_w          = 256,
        diffloss_d          = 4,
        num_sampling_steps  = 1000,
        diffusion_batch_mul = 4,
    )
    loss = model(x)
    print(loss)
    
